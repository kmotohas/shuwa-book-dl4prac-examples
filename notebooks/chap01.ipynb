{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一章 機械学習と深層学習の導入\n",
    "\n",
    "\n",
    "近年、機械学習や深層学習は毎日のようにネットやテレビ等でも取り上げられており、お茶の間レベルで聞かない日はないほどの盛り上がりを見せています。AIという名称でバズワード化されており、現在最も注目度が高いIT技術のひとつでしょう。本章では、機械学習を実践で使う際に必要となる基本的な知識を紹介します。すでに機械学習に関して十分知識をお持ちの方は、本章は読み飛ばしていただいて大丈夫です。それでは早速始めましょう。\n",
    "\n",
    "## 1.1 従来のプログラミングと機械学習との違い\n",
    "\n",
    "機械学習をはじめて触れた方は従来のプログラミングとの違いに戸惑う方がいらっしゃいます。そのためここでは、従来のプログラミングと機械学習の違いを説明します。本章の内容は、Courseraという有名なオンラインの学習サイトで公開されている、deeplearning.aiによる\"Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning\"という講座を参考に構成されています。\n",
    "\n",
    "従来のシステムでは以下の図のようにまず何かの課題に対してひたすらルールを自分でプログラミングします。そして、そのプログラムに対してデータを入れると答えが出てくるという、プログラム視点で見るとある意味受動的な構成になっています。\n",
    "\n",
    "![traditional](figures/chap01_traditional_programming.png)\n",
    "<center>従来のプログラミングの処理の流れ <font color=\"red\">to be replaced</font></center>\n",
    "\n",
    "たとえば人の活動の検知をしたい場合を考えます。\n",
    "\n",
    "歩いているという状態をプログラミングする場合にどうすればよいでしょうか。以下の例は非常に概念的ではありますが、例えば`if`文を用いて、動くスピードが 4 km/h 以下であったら歩いているという風に定義をする方法があります。この方法で走っている状態を定義する場合には、`else`文を加えて 4 km/h 以上だったら走っているというように定義できます。このようなやり方でどんどん機能拡張をしていくことを考えると、自転車に乗っている場合には、さらに`else`文を加えてスピードが 12 km/h 以上だったら自転車に乗っている状態と定義できます。このようにだんだんとやりたいことを増やしていくと、たとえばゴルフをしている状態なども実装したくなるかもしれませんが、どう言う風にそれを実装していいのか複雑すぎてお手上げになってしまいます。このような、人が簡単なロジックで記述することが難しい複雑なタスクに対しては機械学習が用いられるようになってきています。\n",
    "\n",
    "![activity](figures/chap01_activity_recognition.png)\n",
    "<center>人の動作に関する従来のプログラミング例 <font color=\"red\">to be replaced</font></center>\n",
    "\n",
    "先ほどの従来のプログラミングの処理の流れを表した図のように機械学習の処理の流れを図にすると以下のようになります。\n",
    "\n",
    "![machine](figures/chap01_machine_learning.png)\n",
    "<center>機械学習の処理の流れ <font color=\"red\">TODO: replace</font></center>\n",
    "\n",
    "従来のプログラミングでは、ルールを決めてデータを入れてあげると答えが出てくるのでしたが、機械学習ではデータと答えのペアを与えてあげると、アルゴリズムがルールを作ってくれます。ここでのアルゴリズムは統計に基づいて確率的に機械学習モデルのパラメーターを調整します。ある程度説明力を持った手法も存在しますが、機械学習がよくブラックボックスだといわれるゆえんはたぶんこのようなところにあります。\n",
    "たとえば下図のように、歩いている場面を撮った画像に、ラベルとして”歩いています”という属性をタグ付けします。同様に、走っている場面の画像には”走っています”、自転車に乗っている場面の画像には”自転車に乗っています”、ゴルフをしている場面の画像には“ゴルフをしています”というように画像データと答えにあたるラベルのペアをたくさん用意してモデルに与えてあげると、アルゴリズムがどういう風に推論をすればよいかを教えてくれます。\n",
    " \n",
    "![activity_ml](figures/chap01_activity_recognition_ml.png)\n",
    "<center>機械学習による人の動作のプログラミングイメージ <font color=\"red\">to be replaced</font></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 機械学習のさまざまなアプローチ\n",
    "\n",
    "機械学習では使用されるデータセットによる違いや答えの求め方の違いで大きく４つの学習方法に分類されます。用いる学習方法の違いによりアルゴリズムやモデルが異なるため、ここではそれぞれの学習方法の違いを理解しましょう。\n",
    "\n",
    "![ml_types](figures/chap01_ml_types.png)\n",
    "<center>機械学習の学習方法の分類 <font color=\"red\">to be replaced</font></center>\n",
    "\n",
    "機械学習の学習方法の違いを大別すると上の図のように、教師あり、教師なし、半教師あり、強化学習の４種類があります。\n",
    "\n",
    "それぞれの学習は大まかに以下のようになっています。\n",
    "\n",
    "- 教師あり学習：入力と出力の関係を学習するアルゴリズム\n",
    "- 教師なし学習：データの構造を学習するアルゴリズム\n",
    "- 半教師あり学習：教師あり学習と教師なし学習を組み合わせたもの\n",
    "- 強化学習：報酬を最大にするような行動を学習するアルゴリズム\n",
    "\n",
    "最初に以下のピザとドーナッツの画像の例を用いて、教師あり学習、教師なし学習、半教師あり学習ではどのようなことをやっているかについて説明します。\n",
    "\n",
    "![pizza_donut](figures/chap01_pizza_donut.png)\n",
    "<center>機械学習で用いる画像の例</center>\n",
    "\n",
    "### 教師あり学習とは\n",
    "\n",
    "まず教師あり学習と教師なし学習では用いられるデータの形式に違いがあります。\n",
    "教師あり学習では、以下の図のようにピザとドーナッツのそれぞれの画像に答えとなるピザやドーナッツのラベルがペアで用いられます。アルゴリズムに対して正解のラベルを与えることは、先生が生徒に対して正解を教えながら学習を進めていくことに似ているため、教師あり学習と呼ばれています。\n",
    "それに対して、教師なし学習で用いられるデータには、前のピザとドーナッツの画像のように、それぞれの画像に対して正解にあたるピザかドーナッツかのラベルがついていません。あるのは単なる画像データだけです。\n",
    "\n",
    "![pizza_donut_labeled](figures/chap01_pizza_donut_labeled.png)\n",
    "<center>画像のラベリングの例</center>\n",
    "\n",
    "教師あり学習が具体的にやっていることは、以下の図のように、ピザとドーナッツの間の決定境界と呼ばれる、できるだけ正しくピザかドーナッツかを見分けられる線を見つけることです。たくさんの画像から最適な境界線を引くルールを学んでいきます。\n",
    "\n",
    "![pizza_donut_labeled_boundary](figures/chap01_pizza_donut_labeled_boundary.png)\n",
    "<center>教師あり学習の分類のイメージ</center>\n",
    "\n",
    "### 教師なし学習とは\n",
    "\n",
    "教師なし学習には、正解のラベルがついていないため、教師あり学習のように正解から学習することができません。しかし、本来ピザとドーナッツのことを全く知らない人間でも、少なくともなんとなくその形状から「これらは違うものだ」ということは想像できるでしょう。教師なし学習では、こういったことを学んでいきます。この例では、ピザとドーナッツの複数枚の画像からピザの特徴とドーナッツの特徴の違いをとらえていくことで正しく区別できるようにしていきます。図の例のような場合だと、▽のような先がとがっているものがピザで、丸い形で中にももう一つ丸い形が含まれているものがドーナッツというようなルールを、機械学習が画像の特徴として学んでいきます。K平均法などの教師なし学習アルゴリズムではいくつのパターンに分けてあげるかについて、最初に指定する必要があります。つまり今回の例では、画像を２種類に分類するように最初に指定します。今回のケースで4つに分類するように誤った指定をしてしますと正しく分類できないため、教師なし学習では、アルゴリズムを指定する前に、いくつに分けるのかを正しく指定するために、あらかじめ入力すべき画像の特徴を理解しておく必要があります。\n",
    "\n",
    "![pizza_donut_kmeans](figures/chap01_pizza_donut_kmeans.png) \n",
    "<center>教師無し学習の分類のイメージ</center>\n",
    "\n",
    "### 半教師あり学習とは\n",
    "\n",
    "半教師あり学習とは、教師あり学習と教師なし学習を組み合わせたものです。\n",
    "\n",
    "機械学習で学習用のデータを準備する場合、学習させるデータを多量に準備するだけでも手間がかかります。教師あり学習では、それぞれのデータにさらに正解のラベルまで準備する必要があり、教師あり学習のデータを準備するのには非常に労力がかかります。たとえば1万枚のピザとドーナッツの画像があったとします。1万枚の画像に対してピザかドーナッツかの正解のラベルを付けるのは大変手間がかかりますよね。そこで、データの一部であるたとえば1000枚の画像に対して教師ありデータを準備し、それ以外の9000枚のデータは教師なしデータとすることで、正解のラベルを付けるという作業は大幅に減らすことができます。半教師あり学習では、最初にぞれぞれのデータごとに個々の学習を行い、最後にこの２つを組み合わせることで、すべてのデータに対して正しく正解のラベルまで答えられるようにします。これを応用し、半教師あり学習は教師なしデータに対して正解のラベルを付けた教師ありデータにするようなタスクにも利用されています。\n",
    "\n",
    "![pizza_donut_semisupervised](figures/chap01_pizza_donut_semisupervised.png)\n",
    "<center>半教師あり学習の分類のイメージ</center>\n",
    "\n",
    "### 強化学習とは\n",
    "\n",
    "強化学習は、今まで説明したそれぞれの学習方法と違い、予測した結果に対してそれが正しかったか誤っていたかを教えていくことで、できるだけ正しく予測するようにトライアンドエラーを繰り返していくという学習方法になります。実際には報酬と呼ばれる、うまくいけばポイントがあがり、失敗した場合には罰としてポイントが下がるような仕組みで学習を行っていきます。以前話題になった囲碁の例で説明すると、最終的に勝利した対局における手に関しては例えば＋１ポイントを与え、負けた場合は－１ポイントを与えるといった報酬を設計しておきます。すると、何万回と対局を重ねるうちにどういう局面でどういう手を打てば勝利につながるか学習していくことが可能です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 機械学習アルゴリズムのイメージ\n",
    "\n",
    "本節では、機械学習アルゴリズムがどのように学習を進めていくのかについて説明したあと、ディープラーニングの基本モデルを使って実際のプログラミングの流れと学習結果について見ていきます。\n",
    "\n",
    "まず機械学習が具体的にどのようなことをやっているかについて知るために、機械学習のHello Worldにあたる以下の例について説明します。\n",
    " \n",
    "### 機械学習のHello World\n",
    "\n",
    "![hello_world](figures/chap01_ml_hello_world.png)\n",
    "<center>機械学習のHello World</center>\n",
    "\n",
    "上図のxがデータでyが答えとなるデータセットがあり、このxとyの間にどのような関係が成り立つか考えてみます。いかがでしょうか。答えはy = 2x + 1です。人がxとyの関係性を見つけていく場合には例えば、xが1ずつ増えていくとyは2ずつ増えているからy = 2x の関係があると読み取り、つぎにxが0のときyが1なので、+ 1だというような推論をしていきます。このような思考によりxからyへのマッピング、つまり関数を得ることができます。\n",
    "\n",
    "同様の問題を機械学習で解くというのはやや大袈裟ではありますが、試しに流れを見てみましょう。まず下図の青い点のデータに対して、モデルの初期値として適当に緑色の線のような直線を準備します。ここで直線にしている理由は、図を見てわかる通り青の線は基本的に直線のモデルで正しく予測できるだろうという仮定があり、y_ = ax + b の形をした一次関数でａとbがパラメータで表現できるという前提が含まれるからです。\n",
    " \n",
    "![linear_init](figures/chap01_linear_init.png)\n",
    "<center>機械学習の初期状態</center>\n",
    "\n",
    "次に、モデルへのデータ入力で得られた答えと実際に推定したい青のデータ点の間の誤差を計算します。誤差の計算方法にはさまざまなものがありますが、数学的な裏付けがある二乗誤差（MSE: Mean Squared Error）がよく用いられます。下図の赤のそれぞれの矢印線で示される2点の間の長さの２乗の和の平均値を誤差として扱います。実際の青のデータ点に近づけるために、その誤差を小さくするように少しだけパラメータを更新していきます。\n",
    "\n",
    "![linear_mse](figures/chap01_linear_mse.png)\n",
    "<center>機械学習でパラメータ更新のためのアプローチ</center>\n",
    "\n",
    "パラメータを更新したら例えば下図のようになります。\n",
    " \n",
    "![linear_updated](figures/chap01_linear_updated.png)\n",
    "<center>機械学習のパラメータ更新</center>\n",
    "\n",
    "それ以降、再度誤差を計算してパラメータを更新するというプロセスを延々と繰り返すことで、y = 2x + 1にどんどん近づいていくということをやっています。このような背景から、”ディープラーニングは最小二乗法のお化けのようなもの”などとの表現されるのだと思います。\n",
    "今までの説明を、Googleが開発しているTensorFlowのKeras APIというフレームワークを用いてプログラムにしたものが以下に示します。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
    "\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "model.fit(xs, ys, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、実行可能なサンプルプログラムは本書のサンプルプログラムの参照ページからダウンロードできます。本サンプルプログラムをJupyter Notebookなどにインポートして実行してみてください。\n",
    "\n",
    "最初のコードで以下のように使用するモジュールのインポートを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎに先ほどの例のax + b にあたる機械学習のモデルを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上図の例は全結合ニューラルネットワークと呼ばれる形式で、ユニットが１つで入力の型が１のネットワークを定義します。全結合ニューラルネットワークの構成要素はKerasでは`Dense`レイヤーと名付けられています。初期値としては`a`のパラメータには`0.3208431`、`b`には`0`などという適当な値が入ります。\n",
    "実際には実行するたびにランダムな値が入るため、読者の方がサンプルコードを実行される際には、異なるパラメータの値となりますので、ご注意ください。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎにこのモデルをどういう風に最適化していくかを決め、誤差関数を何にするか定義します。最適化手法はここでは基本的な確率的勾配降下法（SGD: Stochastic Gradient Decent）を用いることにします。誤差関数は前述の通り平均二乗誤差 (Mean Squared Error) を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そこで出来上がったモデルに対してデータを入力します。データの形式は図のように配列になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎにfitという関数を呼ぶことで、学習が行われます。誤差（損失、lossとも）が小さくなるように同様の処理が繰り返されていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6 samples\n",
      "Epoch 1/500\n",
      "6/6 [==============================] - 0s 36ms/sample - loss: 5.0377\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 260us/sample - loss: 4.1384\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 3.4273\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 727us/sample - loss: 2.8644\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 693us/sample - loss: 2.4180\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 298us/sample - loss: 2.0634\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 375us/sample - loss: 1.7812\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 625us/sample - loss: 1.5559\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 768us/sample - loss: 1.3755\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 642us/sample - loss: 1.2304\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 2ms/sample - loss: 1.1132\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 1.0180\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 794us/sample - loss: 0.9402\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 616us/sample - loss: 0.8762\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 683us/sample - loss: 0.8230\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 750us/sample - loss: 0.7784\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 531us/sample - loss: 0.7406\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 703us/sample - loss: 0.7082\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 361us/sample - loss: 0.6802\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 930us/sample - loss: 0.6556\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 339us/sample - loss: 0.6338\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 289us/sample - loss: 0.6142\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 273us/sample - loss: 0.5964\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 364us/sample - loss: 0.5801\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 342us/sample - loss: 0.5650\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 916us/sample - loss: 0.5509\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 847us/sample - loss: 0.5376\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 224us/sample - loss: 0.5250\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 362us/sample - loss: 0.5129\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 334us/sample - loss: 0.5014\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 311us/sample - loss: 0.4904\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 5ms/sample - loss: 0.4797\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 556us/sample - loss: 0.4694\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 602us/sample - loss: 0.4594\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 368us/sample - loss: 0.4497\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 633us/sample - loss: 0.4402\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 548us/sample - loss: 0.4310\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 653us/sample - loss: 0.4220\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 626us/sample - loss: 0.4132\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 497us/sample - loss: 0.4046\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 382us/sample - loss: 0.3962\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 411us/sample - loss: 0.3880\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 460us/sample - loss: 0.3800\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 537us/sample - loss: 0.3722\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 632us/sample - loss: 0.3645\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 721us/sample - loss: 0.3570\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 607us/sample - loss: 0.3497\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 408us/sample - loss: 0.3425\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 530us/sample - loss: 0.3354\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 325us/sample - loss: 0.3285\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 626us/sample - loss: 0.3218\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 421us/sample - loss: 0.3152\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 534us/sample - loss: 0.3087\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 279us/sample - loss: 0.3023\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 366us/sample - loss: 0.2961\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 391us/sample - loss: 0.2900\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 441us/sample - loss: 0.2841\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 532us/sample - loss: 0.2782\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 412us/sample - loss: 0.2725\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 626us/sample - loss: 0.2669\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 374us/sample - loss: 0.2614\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 309us/sample - loss: 0.2561\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 372us/sample - loss: 0.2508\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 420us/sample - loss: 0.2457\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 486us/sample - loss: 0.2406\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 724us/sample - loss: 0.2357\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 505us/sample - loss: 0.2308\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 296us/sample - loss: 0.2261\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 447us/sample - loss: 0.2214\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 360us/sample - loss: 0.2169\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 0.2124\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 302us/sample - loss: 0.2081\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 727us/sample - loss: 0.2038\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 459us/sample - loss: 0.1996\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 368us/sample - loss: 0.1955\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 987us/sample - loss: 0.1915\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 254us/sample - loss: 0.1876\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 265us/sample - loss: 0.1837\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 310us/sample - loss: 0.1799\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 386us/sample - loss: 0.1762\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 304us/sample - loss: 0.1726\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 255us/sample - loss: 0.1691\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 329us/sample - loss: 0.1656\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 298us/sample - loss: 0.1622\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 321us/sample - loss: 0.1589\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 310us/sample - loss: 0.1556\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 336us/sample - loss: 0.1524\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 335us/sample - loss: 0.1493\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 389us/sample - loss: 0.1462\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 468us/sample - loss: 0.1432\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 282us/sample - loss: 0.1403\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 349us/sample - loss: 0.1374\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 304us/sample - loss: 0.1346\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 702us/sample - loss: 0.1318\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 346us/sample - loss: 0.1291\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 297us/sample - loss: 0.1264\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 342us/sample - loss: 0.1238\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 259us/sample - loss: 0.1213\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 288us/sample - loss: 0.1188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 480us/sample - loss: 0.1164\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 394us/sample - loss: 0.1140\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 360us/sample - loss: 0.1116\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 455us/sample - loss: 0.1093\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 295us/sample - loss: 0.1071\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 247us/sample - loss: 0.1049\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 234us/sample - loss: 0.1027\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 289us/sample - loss: 0.1006\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 267us/sample - loss: 0.0986\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 355us/sample - loss: 0.0965\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 392us/sample - loss: 0.0946\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 323us/sample - loss: 0.0926\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 284us/sample - loss: 0.0907\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 314us/sample - loss: 0.0889\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 287us/sample - loss: 0.0870\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 433us/sample - loss: 0.0852\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 280us/sample - loss: 0.0835\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 413us/sample - loss: 0.0818\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 419us/sample - loss: 0.0801\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 493us/sample - loss: 0.0785\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 264us/sample - loss: 0.0768\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 350us/sample - loss: 0.0753\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 254us/sample - loss: 0.0737\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 262us/sample - loss: 0.0722\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 281us/sample - loss: 0.0707\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 308us/sample - loss: 0.0693\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 307us/sample - loss: 0.0678\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 487us/sample - loss: 0.0664\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 350us/sample - loss: 0.0651\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 445us/sample - loss: 0.0637\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 335us/sample - loss: 0.0624\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 415us/sample - loss: 0.0612\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 316us/sample - loss: 0.0599\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 252us/sample - loss: 0.0587\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 449us/sample - loss: 0.0575\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 396us/sample - loss: 0.0563\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 0.0551\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 509us/sample - loss: 0.0540\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 8ms/sample - loss: 0.0529\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 601us/sample - loss: 0.0518\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 552us/sample - loss: 0.0507\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 549us/sample - loss: 0.0497\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 483us/sample - loss: 0.0487\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 631us/sample - loss: 0.0477\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 584us/sample - loss: 0.0467\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 361us/sample - loss: 0.0457\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 288us/sample - loss: 0.0448\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 288us/sample - loss: 0.0439\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 354us/sample - loss: 0.0430\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 549us/sample - loss: 0.0421\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 618us/sample - loss: 0.0412\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 437us/sample - loss: 0.0404\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 456us/sample - loss: 0.0395\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 276us/sample - loss: 0.0387\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 287us/sample - loss: 0.0379\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 296us/sample - loss: 0.0372\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 257us/sample - loss: 0.0364\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 889us/sample - loss: 0.0357\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 700us/sample - loss: 0.0349\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 377us/sample - loss: 0.0342\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 343us/sample - loss: 0.0335\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 266us/sample - loss: 0.0328\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 278us/sample - loss: 0.0321\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 212us/sample - loss: 0.0315\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 443us/sample - loss: 0.0308\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 619us/sample - loss: 0.0302\n",
      "Epoch 166/500\n",
      "6/6 [==============================] - 0s 424us/sample - loss: 0.0296\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 362us/sample - loss: 0.0290\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 379us/sample - loss: 0.0284\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 382us/sample - loss: 0.0278\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 216us/sample - loss: 0.0272\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 208us/sample - loss: 0.0267\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 255us/sample - loss: 0.0261\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 272us/sample - loss: 0.0256\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 673us/sample - loss: 0.0251\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 873us/sample - loss: 0.0245\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 425us/sample - loss: 0.0240\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 416us/sample - loss: 0.0235\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 293us/sample - loss: 0.0231\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 342us/sample - loss: 0.0226\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 298us/sample - loss: 0.0221\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 767us/sample - loss: 0.0217\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 374us/sample - loss: 0.0212\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 253us/sample - loss: 0.0208\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 710us/sample - loss: 0.0204\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 250us/sample - loss: 0.0199\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 440us/sample - loss: 0.0195\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 452us/sample - loss: 0.0191\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 352us/sample - loss: 0.0187\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 680us/sample - loss: 0.0184\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 678us/sample - loss: 0.0180\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 852us/sample - loss: 0.0176\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 559us/sample - loss: 0.0172\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 318us/sample - loss: 0.0169\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 229us/sample - loss: 0.0165\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 0.0162\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 267us/sample - loss: 0.0159\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 902us/sample - loss: 0.0155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 276us/sample - loss: 0.0152\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 297us/sample - loss: 0.0149\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 717us/sample - loss: 0.0146\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 280us/sample - loss: 0.0143\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 274us/sample - loss: 0.0140\n",
      "Epoch 203/500\n",
      "6/6 [==============================] - 0s 245us/sample - loss: 0.0137\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 0.0134\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 246us/sample - loss: 0.0132\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 212us/sample - loss: 0.0129\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 0.0126\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 812us/sample - loss: 0.0124\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 222us/sample - loss: 0.0121\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 259us/sample - loss: 0.0119\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 321us/sample - loss: 0.0116\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 253us/sample - loss: 0.0114\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 375us/sample - loss: 0.0112\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 351us/sample - loss: 0.0109\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 220us/sample - loss: 0.0107\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 443us/sample - loss: 0.0105\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 266us/sample - loss: 0.0103\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 230us/sample - loss: 0.0101\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 223us/sample - loss: 0.0098\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 273us/sample - loss: 0.0096\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 197us/sample - loss: 0.0094\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 226us/sample - loss: 0.0093\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 345us/sample - loss: 0.0091\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 247us/sample - loss: 0.0089\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 254us/sample - loss: 0.0087\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 205us/sample - loss: 0.0085\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 275us/sample - loss: 0.0083\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 224us/sample - loss: 0.0082\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 259us/sample - loss: 0.0080\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 241us/sample - loss: 0.0078\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 251us/sample - loss: 0.0077\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 208us/sample - loss: 0.0075\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 257us/sample - loss: 0.0074\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 434us/sample - loss: 0.0072\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 253us/sample - loss: 0.0071\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 217us/sample - loss: 0.0069\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 229us/sample - loss: 0.0068\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 221us/sample - loss: 0.0066\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 232us/sample - loss: 0.0065\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 223us/sample - loss: 0.0064\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 233us/sample - loss: 0.0062\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 230us/sample - loss: 0.0061\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 225us/sample - loss: 0.0060\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 249us/sample - loss: 0.0059\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 265us/sample - loss: 0.0057\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 285us/sample - loss: 0.0056\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 258us/sample - loss: 0.0055\n",
      "Epoch 248/500\n",
      "6/6 [==============================] - 0s 294us/sample - loss: 0.0054\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 247us/sample - loss: 0.0053\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 241us/sample - loss: 0.0052\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 238us/sample - loss: 0.0051\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 210us/sample - loss: 0.0050\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 253us/sample - loss: 0.0049\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 226us/sample - loss: 0.0048\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 216us/sample - loss: 0.0047\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 188us/sample - loss: 0.0046\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 244us/sample - loss: 0.0045\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 217us/sample - loss: 0.0044\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 354us/sample - loss: 0.0043\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 325us/sample - loss: 0.0042\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 292us/sample - loss: 0.0041\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 240us/sample - loss: 0.0040\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 297us/sample - loss: 0.0040\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 306us/sample - loss: 0.0039\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 857us/sample - loss: 0.0038\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 416us/sample - loss: 0.0037\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 305us/sample - loss: 0.0036\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 216us/sample - loss: 0.0036\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 219us/sample - loss: 0.0035\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 263us/sample - loss: 0.0034\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 235us/sample - loss: 0.0033\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 197us/sample - loss: 0.0033\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 276us/sample - loss: 0.0032\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 341us/sample - loss: 0.0031\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 340us/sample - loss: 0.0031\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 381us/sample - loss: 0.0030\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 277us/sample - loss: 0.0030\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 246us/sample - loss: 0.0029\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 261us/sample - loss: 0.0028\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 293us/sample - loss: 0.0028\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 217us/sample - loss: 0.0027\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 230us/sample - loss: 0.0027\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 276us/sample - loss: 0.0026\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 242us/sample - loss: 0.0026\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 428us/sample - loss: 0.0025\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 289us/sample - loss: 0.0025\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 312us/sample - loss: 0.0024\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 265us/sample - loss: 0.0024\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 240us/sample - loss: 0.0023\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 315us/sample - loss: 0.0023\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 234us/sample - loss: 0.0022\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 238us/sample - loss: 0.0022\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 264us/sample - loss: 0.0021\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 238us/sample - loss: 0.0021\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 229us/sample - loss: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 264us/sample - loss: 0.0020\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 289us/sample - loss: 0.0020\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 311us/sample - loss: 0.0019\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 245us/sample - loss: 0.0019\n",
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 234us/sample - loss: 0.0018\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 244us/sample - loss: 0.0018\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 230us/sample - loss: 0.0018\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 275us/sample - loss: 0.0017\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 241us/sample - loss: 0.0017\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 229us/sample - loss: 0.0017\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 416us/sample - loss: 0.0016\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 217us/sample - loss: 0.0016\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 470us/sample - loss: 0.0016\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 242us/sample - loss: 0.0015\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 244us/sample - loss: 0.0015\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 311us/sample - loss: 0.0015\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 199us/sample - loss: 0.0014\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 323us/sample - loss: 0.0014\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 224us/sample - loss: 0.0014\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 246us/sample - loss: 0.0013\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 327us/sample - loss: 0.0013\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 274us/sample - loss: 0.0013\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 296us/sample - loss: 0.0013\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 464us/sample - loss: 0.0012\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 419us/sample - loss: 0.0012\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 237us/sample - loss: 0.0012\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 336us/sample - loss: 0.0011\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 241us/sample - loss: 0.0011\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 215us/sample - loss: 0.0011\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 275us/sample - loss: 0.0011\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 267us/sample - loss: 0.0010\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 317us/sample - loss: 0.0010\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 317us/sample - loss: 0.0010\n",
      "Epoch 330/500\n",
      "6/6 [==============================] - 0s 432us/sample - loss: 9.8340e-04\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 334us/sample - loss: 9.6320e-04\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 209us/sample - loss: 9.4342e-04\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 245us/sample - loss: 9.2403e-04\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 207us/sample - loss: 9.0506e-04\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 270us/sample - loss: 8.8647e-04\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 326us/sample - loss: 8.6826e-04\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 231us/sample - loss: 8.5042e-04\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 295us/sample - loss: 8.3295e-04\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 502us/sample - loss: 8.1585e-04\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 383us/sample - loss: 7.9909e-04\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 209us/sample - loss: 7.8267e-04\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 242us/sample - loss: 7.6659e-04\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 215us/sample - loss: 7.5085e-04\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 338us/sample - loss: 7.3543e-04\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 391us/sample - loss: 7.2032e-04\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 407us/sample - loss: 7.0553e-04\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 282us/sample - loss: 6.9103e-04\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 259us/sample - loss: 6.7684e-04\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 359us/sample - loss: 6.6293e-04\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 449us/sample - loss: 6.4932e-04\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 328us/sample - loss: 6.3598e-04\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 355us/sample - loss: 6.2292e-04\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 262us/sample - loss: 6.1013e-04\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 318us/sample - loss: 5.9759e-04\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 453us/sample - loss: 5.8532e-04\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 542us/sample - loss: 5.7329e-04\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 228us/sample - loss: 5.6152e-04\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 240us/sample - loss: 5.4998e-04\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 314us/sample - loss: 5.3869e-04\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 237us/sample - loss: 5.2762e-04\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 300us/sample - loss: 5.1679e-04\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 247us/sample - loss: 5.0617e-04\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 346us/sample - loss: 4.9577e-04\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 397us/sample - loss: 4.8559e-04\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 253us/sample - loss: 4.7561e-04\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 244us/sample - loss: 4.6584e-04\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 243us/sample - loss: 4.5628e-04\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 289us/sample - loss: 4.4690e-04\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 251us/sample - loss: 4.3772e-04\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 254us/sample - loss: 4.2873e-04\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 290us/sample - loss: 4.1993e-04\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 243us/sample - loss: 4.1130e-04\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 240us/sample - loss: 4.0285e-04\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 471us/sample - loss: 3.9458e-04\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 454us/sample - loss: 3.8647e-04\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 287us/sample - loss: 3.7853e-04\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 276us/sample - loss: 3.7076e-04\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 363us/sample - loss: 3.6314e-04\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 441us/sample - loss: 3.5568e-04\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 220us/sample - loss: 3.4837e-04\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 208us/sample - loss: 3.4122e-04\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 217us/sample - loss: 3.3421e-04\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 313us/sample - loss: 3.2735e-04\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 231us/sample - loss: 3.2062e-04\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 636us/sample - loss: 3.1404e-04\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 472us/sample - loss: 3.0759e-04\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 260us/sample - loss: 3.0127e-04\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 234us/sample - loss: 2.9508e-04\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 241us/sample - loss: 2.8902e-04\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 216us/sample - loss: 2.8309e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 348us/sample - loss: 2.7727e-04\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 200us/sample - loss: 2.7157e-04\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 247us/sample - loss: 2.6600e-04\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 244us/sample - loss: 2.6053e-04\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 278us/sample - loss: 2.5518e-04\n",
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 245us/sample - loss: 2.4994e-04\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 335us/sample - loss: 2.4481e-04\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 478us/sample - loss: 2.3978e-04\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 257us/sample - loss: 2.3485e-04\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 240us/sample - loss: 2.3003e-04\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 214us/sample - loss: 2.2530e-04\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 217us/sample - loss: 2.2068e-04\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 235us/sample - loss: 2.1614e-04\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 236us/sample - loss: 2.1170e-04\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 216us/sample - loss: 2.0736e-04\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 347us/sample - loss: 2.0310e-04\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 238us/sample - loss: 1.9893e-04\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 210us/sample - loss: 1.9484e-04\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 308us/sample - loss: 1.9084e-04\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 600us/sample - loss: 1.8692e-04\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 266us/sample - loss: 1.8308e-04\n",
      "Epoch 412/500\n",
      "6/6 [==============================] - 0s 300us/sample - loss: 1.7932e-04\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 322us/sample - loss: 1.7563e-04\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 367us/sample - loss: 1.7203e-04\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 273us/sample - loss: 1.6849e-04\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 247us/sample - loss: 1.6503e-04\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 211us/sample - loss: 1.6164e-04\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 270us/sample - loss: 1.5832e-04\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 500us/sample - loss: 1.5507e-04\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 521us/sample - loss: 1.5189e-04\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 311us/sample - loss: 1.4876e-04\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 337us/sample - loss: 1.4571e-04\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 380us/sample - loss: 1.4272e-04\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 318us/sample - loss: 1.3979e-04\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 240us/sample - loss: 1.3691e-04\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 289us/sample - loss: 1.3410e-04\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 224us/sample - loss: 1.3134e-04\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 210us/sample - loss: 1.2865e-04\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 521us/sample - loss: 1.2601e-04\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 646us/sample - loss: 1.2342e-04\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 298us/sample - loss: 1.2088e-04\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 316us/sample - loss: 1.1840e-04\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 709us/sample - loss: 1.1597e-04\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 315us/sample - loss: 1.1359e-04\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 256us/sample - loss: 1.1125e-04\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 390us/sample - loss: 1.0897e-04\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 269us/sample - loss: 1.0673e-04\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 323us/sample - loss: 1.0454e-04\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 259us/sample - loss: 1.0239e-04\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 231us/sample - loss: 1.0029e-04\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 251us/sample - loss: 9.8228e-05\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 234us/sample - loss: 9.6210e-05\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 9.4234e-05\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 822us/sample - loss: 9.2298e-05\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 238us/sample - loss: 9.0402e-05\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 235us/sample - loss: 8.8545e-05\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 420us/sample - loss: 8.6726e-05\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 217us/sample - loss: 8.4944e-05\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 220us/sample - loss: 8.3199e-05\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 293us/sample - loss: 8.1490e-05\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 208us/sample - loss: 7.9816e-05\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 437us/sample - loss: 7.8176e-05\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 538us/sample - loss: 7.6571e-05\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 348us/sample - loss: 7.4998e-05\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 210us/sample - loss: 7.3457e-05\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 329us/sample - loss: 7.1949e-05\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 333us/sample - loss: 7.0471e-05\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 262us/sample - loss: 6.9023e-05\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 260us/sample - loss: 6.7605e-05\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 368us/sample - loss: 6.6217e-05\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 250us/sample - loss: 6.4857e-05\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 230us/sample - loss: 6.3524e-05\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 408us/sample - loss: 6.2220e-05\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 274us/sample - loss: 6.0940e-05\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 621us/sample - loss: 5.9689e-05\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 208us/sample - loss: 5.8463e-05\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 213us/sample - loss: 5.7262e-05\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 273us/sample - loss: 5.6086e-05\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 275us/sample - loss: 5.4934e-05\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 274us/sample - loss: 5.3805e-05\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 236us/sample - loss: 5.2700e-05\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 283us/sample - loss: 5.1617e-05\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 296us/sample - loss: 5.0557e-05\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 1ms/sample - loss: 4.9519e-05\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 306us/sample - loss: 4.8502e-05\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 263us/sample - loss: 4.7506e-05\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 235us/sample - loss: 4.6530e-05\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 261us/sample - loss: 4.5575e-05\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 385us/sample - loss: 4.4637e-05\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 286us/sample - loss: 4.3722e-05\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 578us/sample - loss: 4.2823e-05\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 503us/sample - loss: 4.1944e-05\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 292us/sample - loss: 4.1082e-05\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 666us/sample - loss: 4.0238e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 3.9412e-05\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 310us/sample - loss: 3.8603e-05\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 213us/sample - loss: 3.7810e-05\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 319us/sample - loss: 3.7033e-05\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 406us/sample - loss: 3.6272e-05\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 631us/sample - loss: 3.5527e-05\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 332us/sample - loss: 3.4797e-05\n",
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 223us/sample - loss: 3.4082e-05\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 226us/sample - loss: 3.3383e-05\n",
      "Epoch 494/500\n",
      "6/6 [==============================] - 0s 348us/sample - loss: 3.2696e-05\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 300us/sample - loss: 3.2025e-05\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 229us/sample - loss: 3.1368e-05\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 314us/sample - loss: 3.0724e-05\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 264us/sample - loss: 3.0093e-05\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 331us/sample - loss: 2.9474e-05\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 286us/sample - loss: 2.8868e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13bb63790>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, ys, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.984324]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([10.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上図のように最終的に学習が終わったモデルに対して、`10.0`という値を入力してあげると、正解値の19に近い`18.989324`という値が返ってきていることが理解いただけると思います。\n",
    "ただしニューラルネットワークは確率的にパラメータを更新していくため、完全に19という正解値に到達することはほぼありませんが、今回の例で、学習が進んでいく様子を理解いただけたと思います。\n",
    "今回のサンプルプログラムではKerasを用いていますが、Kerasにかぎらず近年の主なディープラーニングの開発フレームワークを使うことで、今回の例のように簡単にモデルを定義することができます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 全結合ニューラルネットワークの学習\n",
    "\n",
    "ディープラーニングのモデルにもさまざまなものが存在しますが、まず一番基本的な全結合ニューラルネットワーク (Fully Connected Neural Network) について説明します。2章以降で紹介するいくつかのアルゴリズムの理解の助けにもなる情報ですのでしっかり理解しましょう。\n",
    "\n",
    "![fully_connected](figures/chap01_fully_connected.png) \n",
    "<center>全結合ニューラルネットワーク <font color=\"red\">TODO: replaced</font></center>\n",
    "\n",
    "ニューラルネットワークの解説では上図のようなニューロン同士が密に結合している図がよく出てくると思いますが、この図のように表現されるニューラルネットワークが全結合ニューラルネットワークと呼ばれるものです。内部で何を行っているかというと、単純に「重み」×「x」＋「バイアス」のような ax + bを多次元的に計算しているだけです。数学的に言うと、線形的に結合して和をとっているだけです。\n",
    "\n",
    "さきほど最適化のためのアルゴリズムを設定すると説明しましたが、ソースコードにあった`sgd`がどのようなものかを説明します。この最適化手法は確率的勾配降下法（SGD）と呼ばれるものです。ディープラーニングの世界で最適化のアルゴリズムとしてデファクトスタンダードとなっています。\n",
    "\n",
    "最適化を行っていくには重みなどのパラメータを更新していきますが、具体的な重みの更新を数式で表したものが図のようにとなります。それは誤差関数（損失関数、loss functionとも) のパラメータでの微分をとってさらに学習率と呼ばれる適当な係数をかけて、その値を新しい重みの値にすることを表しています。\n",
    "\n",
    "![sgd](figures/chap01_sgd.png) \n",
    "<center>最適化アルゴリズムの選択 <font color=\"red\">TODO: replaced</font></center>\n",
    "\n",
    "この数式が具体的にどのようなことをやっているかについて上の図を用いて説明します。図の青の曲線が、重みパラメータ`w`に対する誤差関数だと思ってください。たとえばパラメータが図の一番左端の点にあるとき、その曲線の傾きを計算すると負の値をとります。誤差を最小にするには、パラメータが図の曲線の一番へこんだ場所にいって欲しいわけです。つまり、左端の赤い点から右側に移動してくれればよいのです。この図では、左端の赤い点における傾きは右下がり、つまり負の値になっています。重みを右に動かしたい場合は負の傾きの値を引き算してあげれば重みの値が大きくなる、つまり赤い点が右側へ移動していきます。\n",
    "\n",
    "実際に最適化を実行していく場合、確率的勾配降下法はあまりにもシンプルな手法なので、その改良版として、モーメンタム、Nesterovの加速法、RSMProp、Adamなどさまざまなものが考案されています。2019年の3月にも国際会議ICLR2019で採択された最適化のアルゴリズムとしてAdaBoundが出てくるなど、日々研究が進んで進化している分野ですので、新たなものについても情報を収集するようにぜひ心がけてください。ただし基本的にはある程度高速にパラメータが収束することが経験的に知られているAdamを使えばよいと思います。たいていのディープラーニングフレームワークではAdamは簡単に用いることができます。Kerasの場合は先程のコードの`’sgd’`の部分を`’adam’`と書き換えればOKです。\n",
    "\n",
    "### 勾配降下法のイメージ\n",
    "![sgd](figures/chap01_sgd_image.png) \n",
    "<center>勾配降下法のイメージ <font color=\"red\">TODO: replaced</font></center>\n",
    "\n",
    "今までの説明内での目的は誤差の最小化でしたが正負の符号を変えるだけで最大化問題になるので、次の説明では最大化問題を解きたいと思って読み進めてください。\n",
    "\n",
    "勾配降下法とは「目隠しで足元の勾配の感覚のみを頼りに山の頂上を目指すようなもの」と喩えることができます。頂上を目指す場合には、目を閉じてそれぞれ場所の足元の感覚から上り坂だと思われる方に進むということを延々と繰り返すようなものだということです。図の数式に出てくるギリシャ文字のη（イータ）という記号は、学習率と呼ばれるパラメータですが、これは歩幅のイメージです。たとえばこのηの値が0.001のように非常に小さい値の場合には、すり足のようにゆっくり慎重に進んで、大きな値であれば大股で進んでいくという感じです。誤差関数は大抵きれいな二次曲線の形をしているわけではありません。実際の山のように、ある一帯で見た場合には最小値となる、極小値と呼ばれる局所的に一番低い場所がたくさんあるゴツゴツとした形になっているでしょう。そのため目標となる最小値の場所ではない極小値のところで落ち着かないように、学習し始めのうちはそういう場所は気にせず飛び越えてもらいたいのです。それを実現するために、最初は学習率を大きめにして大股で歩いて行って、頂上に近づいてきたら学習率を小さい値にして歩幅を小さくするようにスケジューリングして実行していくということがよく行われています。\n",
    "\n",
    "### 使用するデータセットの紹介\n",
    "\n",
    "次の章で説明するCNNの話に近づいていくために、Fashion MNISTデータセットというものをここで紹介します。ディープラーニングの基礎的な話を聞く場合、手書き文字の認識の話がよく話題になると思います。Fashion MNISTのデータセットは手書き文字より少し複雑になったもので、7万枚の画像データが含まれています。以下の図のようにTシャツ、コート、バッグ、スニーカー、などファッショングッズに関する10カテゴリのピクトグラムのような小さな画像が準備されています。それぞれの画像の大きさは28×28ピクセルで、よく実験やベンチマークで用いられるデータセットです。\n",
    " \n",
    "![fashion_mnist](figures/chap01_fashion_mnist.png)\n",
    "<center>Fashion MNISTデータセット</center>\n",
    "\n",
    "このデータセットを使って、これから全結合ニューラルネットワークによる各カテゴリに分類するモデルを作っていきます。\n",
    "\n",
    "### 全結合ニューラルネットワークの実行例\n",
    "\n",
    "下のサンプルコードを参照しながら全結合ニューラルネットワークの具体的な実装について説明します。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.5611 - accuracy: 0.8103 - val_loss: 0.4770 - val_accuracy: 0.8273\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 1s 22us/sample - loss: 0.4121 - accuracy: 0.8551 - val_loss: 0.4213 - val_accuracy: 0.8509\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 1s 20us/sample - loss: 0.3677 - accuracy: 0.8688 - val_loss: 0.3857 - val_accuracy: 0.8641\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3427 - accuracy: 0.8781 - val_loss: 0.3819 - val_accuracy: 0.8630\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3226 - accuracy: 0.8841 - val_loss: 0.3604 - val_accuracy: 0.8712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18ec5ba50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images / 255.\n",
    "test_images = test_images / 255.\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=128, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "以下ではサンプルコードで行われている処理の概要について順を追って説明をします。\n",
    "\n",
    "サンプルコードの最初の部分では、機械学習用にtensorflowのKeras APIや可視化用のmatplotlibと呼ばれるモジュールをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にデータセットをロードします。Fashion MNISTデータセットは非常に有名なものでよく使われるため、Kerasにも含まれています。ここではTensorFlowのライブラリを使ってデータセットを読み込みます。そうすることで、訓練用の画像と訓練用のラベルのペアが多次元配列に含まれたものが事前に準備され、さらに性能評価用にテスト用の画像とテスト用のラベルのペアも用意されます。こちらのデータセットの画像はカラーではなくグレースケールなので、画像の行列の中身は各ピクセルに対して0（黒）～255（白）の範囲の輝度と呼ばれる明るさを数値化したものが入っています。このままの値を用いてしまうと、数値が大きすぎて訓練がうまくいかなくなる場合があるため、数値を0～1の間の値に変換します。このようにデータを利用しやすい形に整理したり変換したりすることは正規化と呼ばれます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.Fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images / 255.\n",
    "test_images = test_images / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下で具体的な画像の中身がどのようになっているか確認します。\n",
    "\n",
    "train_imagesという訓練用の画像は、6万個の画像があって、高さ28ピクセル、幅28ピクセルの形の配列になっています。ラベルの方は同じく6万個あって0～9の10個の値のどれかが入っていて、たとえば0番目の画像では9つまりアンクルブーツというラベルがついていることを意味しています。\n",
    "\n",
    "実際に最初の画像を可視化してみると以下のように非常に粗いモザイクがかかったようなブーツであることがわかります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のデータセットに対してモデルを組み立てていきましょう。\n",
    "\n",
    "まず28×28ピクセルの形の画像を全結合ニューラルネットワークに入力する必要があるため、以下のように`Flatten`と呼ばれる操作で1列の784次元のベクトルに変換します。具体的には二次元行列の0番目の行の要素を縦に並べ、続けて1番目の行のデータをまた縦に並べるという作業を28行分繰り返して、全部で1列の784次元のデータにします。今回のモデルも全結合ニューラルネットワークで構成します。図のように、128個のニューロン（ユニット）を中間の層に配置して、その後ろに分類を行うための10個のニューロンを配置するというモデルを作っています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dnn_input](figures/chap01_dnn_input.png)\n",
    "<center>画像データのニューラルネットワークへの入力イメージ</center>\n",
    "\n",
    "こちらのコードのようにKerasはわかりやすくそれぞれのモジュールを配列として3つ並べるだけで図のようなニューラルネットワークが完成します。\n",
    "\n",
    "つぎにこちらのモデルに対して最適化のアルゴリズムとしてはAdamを用いることにします。損失関数としては詳しい説明は割愛しますが、分類の問題の損失関数を評価する際に一般的に用いられる交差エントロピー (cross entropy) というものを用います。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今までのプログラムで設定したデータセットとモデルを使って実際に学習を実行させるために、前に説明した`fit`というメソッドを用いて`train_images`と`train_labels`をデータを入力として、訓練を行います。実際にこちらを実行すると、だんだんと正解率 (accuracy) が上がっていっているのが確認できます。このようにある程度ループを回すことで、たとえばドレスの画像を入れたら87パーセントの確率でこれはドレスですといったことを推論してくれるようなモデルができあがります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128, verbose=1, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つぎに評価がどのようになされるかをイメージしていただくために、混合行列（confusion matrix）について説明します。混合行列とはどれをどれくらい正解してどれくらい間違っているかを確かめるための表です。\n",
    "\n",
    "以下の図が混合行列をヒートマップ化したものです。図の横軸が正解のラベル、縦はモデルが予測したラベルを示しています。正解ラベルが0で予測も0であれば正しく予測できている、というような行列になっていて、図全体でみてみると、対角線上にある値が正解した数で、それ以外の値が間違った数になっています。対角線以外は、それぞれどの正解ラベルがどのラベルと誤って予測されたのかが示されています。どこらへんに問題があるのかとかどこらへんが間違いやすいのかについて、混合行列のそれぞれのラベルを見ながら探っていくことができます。\n",
    "\n",
    "<center>混合行列の可視化例 <font color=\"red\">to be added</font></center>\n",
    "\n",
    "### 全結合ニューラルネットワークを画像に用いる場合の欠点\n",
    "\n",
    "全結合ニューラルネットワークを画像に用いる場合の欠点について触れておきます。全結合ニューラルネットワークは、入力ベクトルの全要素のつながりを見ています。画像の場合には基本的に、例えば左上側に位置するピクセルに対して右下側のピクセルにはあまり関係性がなく画像の性質で考えると無視してほしいものになりますが、全結合ニューラルネットワークではそのような無関係なものまで関係性を見つけようと試みます。学習していく中で、あるニューロンと別のニューロンには関係性がないのでつながりの重みがゼロであるといようにもともと明らかであった無意味なパラメータまで頑張って更新していくことになります。このようなやり方は計算コストがかかるのと、そもそもあまりにも相関が複雑になりすぎて学習が進まないということが起こりえます。全結合ニューラルネットワークは住宅の価格予測のようにすでに作成済みの特徴量の組み合わせでこの家の価格はいくらくらいになるのかを予測するときには、明確にそれぞれの特徴量に相関を比較的簡単に見つけられるので有効ですが、画像ではそうはいきません。画像ではむしろ各ピクセル間の大域的な関係性よりも、角が丸っこいだとかシャープだとか局所的な情報の方が特徴を掴むのに重要です。ピクセルの近い部分の関係性を学習し画像の特徴量を抽出するのに適したアルゴリズムとして代表的なものに畳み込みニューラルネットワーク（Convolutional Neural Network: CNN）というものがあります。\n",
    "\n",
    "では早速次章で畳み込みネットワークについて見ていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
